   

<!doctype html>

<html>

<head>

<meta charset='UTF-8'><meta name='viewport' content='width=device-width initial-scale=1'>

<title>彭万红的网站</title><link href='http://fonts.googleapis.com/css?family=Open+Sans:400italic,700italic,700,400&subset=latin,latin-ext' rel='stylesheet' type='text/css' /><style type='text/css'>html {overflow-x: initial !important;}:root { --bg-color: #ffffff; --text-color: #333333; --code-block-bg-color: inherit; }

html { font-size: 14px; background-color: var(--bg-color); color: var(--text-color); font-family: "Helvetica Neue", Helvetica, Arial, sans-serif; -webkit-font-smoothing: antialiased; }

body { margin: 0px; padding: 0px; height: auto; bottom: 0px; top: 0px; left: 0px; right: 0px; font-size: 1rem; line-height: 1.42857; overflow-x: hidden; background: inherit; }

a:active, a:hover { outline: 0px; }

.in-text-selection, ::selection { background: rgb(181, 214, 252); text-shadow: none; }

#write { margin: 0px auto; height: auto; width: inherit; word-break: normal; word-wrap: break-word; position: relative; padding-bottom: 70px; white-space: pre-wrap; overflow-x: visible; contain: layout paint; }

.for-image #write { padding-left: 8px; padding-right: 8px; }

body.typora-export { padding-left: 30px; padding-right: 30px; }

@media screen and (max-width: 500px) {

  body.typora-export { padding-left: 0px; padding-right: 0px; }

  .CodeMirror-sizer { margin-left: 0px !important; }

  .CodeMirror-gutters { display: none !important; }

}






</style>

</head>

<body class='typora-export' >

<div  id='write'  class = 'is-node'><h1><a name='header-n370' class='md-header-anchor '></a>斯坦福大学2014机器学习教程中文笔记目录</h1><ul><li><a href='week1.html' target=_blank>第一周</a></li></ul><p>一、 引言(Introduction) </p><p>1.1 欢迎 </p><p>1.2 机器学习是什么？ </p><p>1.3 监督学习 </p><p>1.4 无监督学习 </p><p>二、单变量线性回归(Linear Regression with One Variable) </p><p>2.1 模型表示 </p><p>2.2 代价函数 </p><p>2.3 代价函数的直观理解I </p><p>2.4 代价函数的直观理解II </p><p>2.5 梯度下降 </p><p>2.6 梯度下降的直观理解 </p><p>2.7 梯度下降的线性回归 </p><p>2.8 接下来的内容 </p><p>三、线性代数回顾(Linear Algebra Review) </p><p>3.1 矩阵和向量 </p><p>3.2 加法和标量乘法 </p><p>3.3 矩阵向量乘法 </p><p>3.4 矩阵乘法 </p><p>3.5 矩阵乘法的性质 </p><p>3.6 逆、转置</p><ul><li><a href='week2.html' target=_blank>第二周</a></li></ul><p>四、多变量线性回归(Linear Regression with Multiple Variables) </p><p>4.1 多维特征 </p><p>4.2 多变量梯度下降 </p><p>4.3 梯度下降法实践1-特征缩放 </p><p>4.4 梯度下降法实践2-学习率 </p><p>4.5 特征和多项式回归 </p><p>4.6 正规方程 </p><p>4.7 正规方程及不可逆性（选修） </p><p>五、Octave教程(Octave Tutorial) </p><p>5.1 基本操作 </p><p>5.2 移动数据 </p><p>5.3 计算数据 </p><p>5.4 绘图数据 </p><p>5.5 控制语句：for，while，if语句 </p><p>5.6 向量化 88</p><p>5.7 工作和提交的编程练习 </p><ul><li><a href='week3.html' target=_blank>第三周</a></li></ul><p>六、逻辑回归(Logistic Regression) </p><p>6.1 分类问题 </p><p>6.2 假说表示 </p><p>6.3 判定边界 </p><p>6.4 代价函数 </p><p>6.5 简化的成本函数和梯度下降 </p><p>6.6 高级优化 </p><p>6.7 多类别分类：一对多 </p><p>七、正则化(Regularization) </p><p>7.1 过拟合的问题 </p><p>7.2 代价函数 </p><p>7.3 正则化线性回归 </p><p>7.4 正则化的逻辑回归模型 </p><ul><li><a href='week4.html' target=_blank>第四周</a></li></ul><p>第八、神经网络：表述(Neural Networks: Representation) </p><p>8.1 非线性假设 </p><p>8.2 神经元和大脑 </p><p>8.3 模型表示1 </p><p>8.4 模型表示2 </p><p>8.5 样本和直观理解1 </p><p>8.6 样本和直观理解II </p><p>8.7 多类分类 </p><ul><li><a href='week5.html' target=_blank>第五周</a></li></ul><p>九、神经网络的学习(Neural Networks: Learning) </p><p>9.1 代价函数 </p><p>9.2 反向传播算法 </p><p>9.3 反向传播算法的直观理解 </p><p>9.4 实现注意：展开参数 </p><p>9.5 梯度检验 </p><p>9.6 随机初始化 </p><p>9.7 综合起来 </p><p>9.8 自主驾驶 </p><ul><li><a href='week6.html' target=_blank>第六周</a></li></ul><p>十、应用机器学习的建议(Advice for Applying Machine Learning) </p><p>10.1 决定下一步做什么 </p><p>10.2 评估一个假设 </p><p>10.3 模型选择和交叉验证集 </p><p>10.4 诊断偏差和方差 </p><p>10.5 正则化和偏差/方差 </p><p>10.6 学习曲线 </p><p>10.7 决定下一步做什么 </p><p>十一、机器学习系统的设计(Machine Learning System Design) </p><p>11.1 首先要做什么 </p><p>11.2 误差分析 </p><p>11.3 类偏斜的误差度量 </p><p>11.4 查准率和查全率之间的权衡 </p><p>11.5 机器学习的数据 </p><p><a href='week7.html' target=_blank>第7周</a></p><p>十二、支持向量机(Support Vector Machines) </p><p>12.1 优化目标 </p><p>12.2 大边界的直观理解 </p><p>12.3 数学背后的大边界分类（选修） </p><p>12.4 核函数1 </p><p>12.5 核函数2 </p><p>12.6 使用支持向量机 </p><ul><li><a href='week8.html' target=_blank>第八周</a></li></ul><p>十三、聚类(Clustering) </p><p>13.1 无监督学习：简介 </p><p>13.2 K-均值算法 </p><p>13.3 优化目标 </p><p>13.4 随机初始化</p><p>13.5 选择聚类数 </p><p>十四、降维(Dimensionality Reduction) </p><p>14.1 动机一：数据压缩 </p><p>14.2 动机二：数据可视化 </p><p>14.3 主成分分析问题 </p><p>14.4 主成分分析算法 </p><p>14.5 选择主成分的数量 </p><p>14.6 重建的压缩表示 </p><p>14.7 主成分分析法的应用建议 </p><ul><li><a href='week9.html' target=_blank>第九周</a></li></ul><p>十五、异常检测(Anomaly Detection) </p><p>15.1 问题的动机 </p><p>15.2 高斯分布 </p><p>15.3 算法 </p><p>15.4 开发和评价一个异常检测系统 </p><p>15.5 异常检测与监督学习对比 </p><p>15.6 选择特征 </p><p>15.7 多元高斯分布（选修） </p><p>15.8 使用多元高斯分布进行异常检测（选修） </p><p>十六、推荐系统(Recommender Systems) </p><p>16.1 问题形式化 </p><p>16.2 基于内容的推荐系统 </p><p>16.3 协同过滤 </p><p>16.4 协同过滤算法 </p><p>16.5 向量化：低秩矩阵分解 </p><p>16.6 推行工作上的细节：均值归一化 </p><ul><li><a href='week10.html' target=_blank>第十周</a></li></ul><p>十七、大规模机器学习(Large Scale Machine Learning) </p><p>17.1 大型数据集的学习 </p><p>17.2 随机梯度下降法 </p><p>17.3 小批量梯度下降 </p><p>17.4 随机梯度下降收敛 </p><p>17.5 在线学习 </p><p>17.6 映射化简和数据并行 </p><p>十八、应用实例：图片文字识别(Application Example: Photo OCR) </p><p>18.1 问题描述和流程图</p><p>18.2 滑动窗口 </p><p>18.3 获取大量数据和人工数据 </p><p>18.4 上限分析：哪部分管道的接下去做 </p><p>十九、总结(Conclusion) </p><p>19.1 总结和致谢 </p><hr /><p><strong>机器学习视频下载链接：<a href='http://pan.baidu.com/s/1dFCQvxZ' target='_blank' >http://pan.baidu.com/s/1dFCQvxZ</a> 密码：dce8</strong></p><p><strong>包含mp4视频和字幕</strong></p><p><strong><a href='index.html' target=_blank>笔记在线阅读</a></strong></p><p><strong><a href='%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%AA%E4%BA%BA%E7%AC%94%E8%AE%B0%E5%AE%8C%E6%95%B4%E7%89%88v5.pdf'>笔记pdf版本下载</a></strong></p><p><strong>机器学习qq群：554839127</strong></p></div>

<script src="https://s13.cnzz.com/z_stat.php?id=1267346656&web_id=1267346656" language="JavaScript"></script>

</body>

</html>
